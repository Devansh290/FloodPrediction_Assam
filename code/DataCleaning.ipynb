{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1dbeb0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully!\n",
      "                    UEI           Start Date             End Date  \\\n",
      "0  UEI-DFO-FL-1985-0001  1985-06-23 00:00:00  1985-06-27 00:00:00   \n",
      "1  UEI-DFO-FL-1985-0002  1985-07-18 00:00:00  1985-07-30 00:00:00   \n",
      "2  UEI-DFO-FL-1985-0003  1985-09-13 00:00:00  1985-10-15 00:00:00   \n",
      "3  UEI-DFO-FL-1985-0004  1985-10-18 00:00:00  1985-10-20 00:00:00   \n",
      "4  UEI-DFO-FL-1986-0001  1986-06-15 00:00:00  1986-08-20 00:00:00   \n",
      "\n",
      "   Duration(Days)        Main Cause Location Districts State  Latitude  \\\n",
      "0             4.0        Heavy rain      NaN       NaN   NaN   14.5398   \n",
      "1            12.0        Heavy rain      NaN       NaN   NaN   32.8353   \n",
      "2            32.0        Heavy rain      NaN       NaN   NaN   26.4816   \n",
      "3             2.0  Tropical cyclone      NaN       NaN   NaN   21.1495   \n",
      "4            66.0    Monsoonal rain      NaN       NaN   NaN   25.6339   \n",
      "\n",
      "   Longitude  Severity  Area Affected  Human fatality Human injured  \\\n",
      "0    75.0937       1.0      254234.60            95.0           NaN   \n",
      "1    76.9103       1.0      117441.17           340.0           NaN   \n",
      "2    82.8434       2.0       89994.67           557.0           NaN   \n",
      "3    86.7154       2.0       46758.64            49.0           NaN   \n",
      "4    84.0726       1.0      507167.44           258.0           NaN   \n",
      "\n",
      "   Human Displaced  Animal Fatality Description of Casualties/injured  \\\n",
      "0          25000.0              NaN                               NaN   \n",
      "1          20000.0              NaN                               NaN   \n",
      "2         152000.0              NaN                               NaN   \n",
      "3         150000.0              NaN                               NaN   \n",
      "4        3040000.0              NaN                               NaN   \n",
      "\n",
      "  Extent of damage  Event Source Event Souce ID  \n",
      "0               NaN          DFO             27  \n",
      "1               NaN          DFO             33  \n",
      "2               NaN          DFO             53  \n",
      "3               NaN          DFO             61  \n",
      "4               NaN          DFO             93  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = \"Copy of IndianFloodInventroy(IFI)(1).xlsx\"\n",
    "\n",
    "# Read the Excel file into a DataFrame\n",
    "try:\n",
    "    data = pd.read_excel(file_path, engine='openpyxl')\n",
    "    print(\"File loaded successfully!\")\n",
    "    \n",
    "    # Display the first few rows of the data\n",
    "    print(data.head())\n",
    "except FileNotFoundError:\n",
    "    print(\"File not found. Please check the file path.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75c9c652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main causes and their counts:\n",
      "heavy rains                                                                2628\n",
      "severe                                                                      210\n",
      "heavy rains                                                                 180\n",
      "flash flood                                                                 159\n",
      "Monsoonal rain                                                              159\n",
      "                                                                           ... \n",
      "moderate(tiata, torsa, raidak. mansi and dalong)                              1\n",
      "severe(ganga , yamuna, ken shandrawal, rapti, sarju, gadri and ghagra )       1\n",
      "heavyh rains                                                                  1\n",
      "moderate(rapti)                                                               1\n",
      "heavy rains & flood in mahananda river                                        1\n",
      "Name: Main Cause, Length: 405, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "main_cause_counts = data['Main Cause'].value_counts()\n",
    "\n",
    "print(\"Main causes and their counts:\")\n",
    "print(main_cause_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07d2fa24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All unique values in 'Main Cause' column:\n",
      "Heavy rain\n",
      "Tropical cyclone\n",
      "Monsoonal rain\n",
      "Torrential rain\n",
      "Heavy Rain\n",
      "Tropical Cyclone\n",
      "Extra-tropical cyclone\n",
      "Dam/Levy, break or release\n",
      "Riverine flood\n",
      "nan\n",
      "Flash flood\n",
      "Coastal flood\n",
      "flood\n",
      "heavy rains, floods\n",
      "floods\n",
      "cloud burst\n",
      "severe floods\n",
      "heavy rains\n",
      "torrential rain\n",
      "heavy rains and floods\n",
      "flooded\n",
      "moderate floods\n",
      "severe flood\n",
      "inceasant rains \n",
      "floods, heavy landslides\n",
      "incessant rains\n",
      "torential rain and heavy floods\n",
      "flash floods\n",
      "moderate to severe\n",
      "moderate\n",
      "severe\n",
      "severe, moderate\n",
      "very severe\n",
      "floods and cloud burst\n",
      "moderate to severe, moderate\n",
      "severe, moderate to severe\n",
      "moderate, severe\n",
      "flash floods in lohit river\n",
      "severe floods \n",
      "moderate to severe floods\n",
      "erratic rains\n",
      "incessant heavy rains\n",
      "heavy rains and moderate floods\n",
      "heavy rains, moderate floods\n",
      "severe floods, heavy rains\n",
      "modern floods, flash floods, heavy rains\n",
      "flash floods due to sudden cloud burst\n",
      "flash floods in vamsadhara river\n",
      "floods in pennar river \n",
      "brahmaputra and its tributaries in moderate floods\n",
      "brahmaputra and its tributaries in floods\n",
      "torrential rains\n",
      "kosi, kamala, ganga and mahananda rivers in floods\n",
      "flash flood in ghaggar river\n",
      "moderate floods in chenab and tawi rivers\n",
      "flash floods in rambhan stream by a cloud burst\n",
      "cauvery and kabini rivers in floods\n",
      "heavy rains following cloud burst\n",
      "flash floods in basrur river\n",
      "continuous heavy rains\n",
      "narmada river in spate\n",
      "flood in waimganga\n",
      "unseasonal rains\n",
      "floods in purna and chandrabhaga\n",
      "wardha and its tributaries  in floods due to continuus heavy rains\n",
      "budki river in floods\n",
      "severe floods in rivulets\n",
      "severe floods in dhundh river.\n",
      "palar river in floods\n",
      "manu and deo rivers in floods\n",
      "rivers gomti, ghagra and tapti in floods\n",
      "rohini, rapti, burhi gandak, sharda, ghagra in floods\n",
      "floods in ghagra, gandak and ganga rivers\n",
      "floods in ghagra, rapti , burhi and kawano\n",
      "flodds in rapti, burhi and ghagra rivers\n",
      "unseasonal heavy rains\n",
      "mechi, jalanhaka and mahananda river in floods\n",
      "haldi river in floods\n",
      "severe (brahmaputra and its tributaries)\n",
      "moderate(brahmaputra)\n",
      "moderate (gandak river)\n",
      "moderate (ganga river) \n",
      "severe (ganga, mahananda and pun pun rivers)\n",
      "heavy rains (unseasonal)\n",
      "heavy rains (seasonal)\n",
      "heavy rains (un seasonal)\n",
      "heavy rains, (flooding of tawi river.)\n",
      "moderate (tunga river) \n",
      "severe (ne travathi, sharavathi, aghana shini. & varada rivers.\n",
      "moderate (tungabhadra - river)\n",
      "moderate (streams from cape comorin) ... \n",
      "moderate (chambal river)\n",
      "severe (mahanadi river)\n",
      "heavy rains (unseasonal) \n",
      "severe(unprecedented flooding of shastri and tonvi rivers)\n",
      "moderate (bondi river)\n",
      "moderate (mahanadi river)\n",
      "severe (mahanadi, brahmani, baitarni & devi rivers) \n",
      "moderate (rivulets in spate) \n",
      "moderate(chambal,  parbati  and  rivulets)\n",
      "moderate(chambal and parbati rivers)\n",
      "flash flood(tambraparni river)\n",
      "flash floods in a nullah\n",
      " severe(ganga, yamuna and their tributaries, ghagra, rapti, gomti,burhi-gandak and betwa) )\n",
      "moderate(padma river)\n",
      "moderate(bhagirathi and bhairab rivers)\n",
      "severe (brahmaputra river)\n",
      "severe (godavari)\n",
      "severe(brahmaputra and its tributaries) \n",
      "severe(brahmaputra, barak and their tributaries)\n",
      "severe(barak and its tributaries) \n",
      "severe (mahananda, ganga and kosi)\n",
      "(severe (mahananda, ankan and their tributaries )\n",
      "severe (bagmati)\n",
      "severe(kosi, kamalabalan and gandak)\n",
      "severe(ganga and its tributaries)\n",
      "severe (ganga)\n",
      "severe (ganga, bagmati and their tributaries)\n",
      "severe (ganga, mahananda and their tributaries)\n",
      "severe (ujen and ojat)\n",
      "severe(flash flooding in kalva)\n",
      "severe ( flash flooding in western jamuna canal)\n",
      "severe(flash flooding)\n",
      "heavy rains due to cloud burst\n",
      "severe (krishna, ghatprabha and vedganga)\n",
      "moderate(bhima)\n",
      "moderate (godavari)\n",
      "moderate(berna)\n",
      "severe(berna, bina, ghasan and sunar)\n",
      "moderate (tamus)\n",
      "severe (betwa and kunwari)\n",
      "severe (kalawli, vashishti and shastri)\n",
      "severe(godavari, krishna, bhima, pravara, koyna, manjura, manwad, lendi, kayadu, penganga and warna)\n",
      "severe (ulhas and batsa-kalu)\n",
      "severe (flash flooding in mula-mutha)\n",
      "severe (flash flooding in bhima)\n",
      "severe( godavari, dudhana and poorna)\n",
      "severe (godavari, manjira, lendi and manya)\n",
      "severe(mula-mutha)\n",
      "moderate(kolab and indravati)\n",
      "moderate(brahmi and baitarni)\n",
      "moderate(patialwi rivulet)\n",
      "moderate(ghaggar and tangri)\n",
      "severe (flash floods)\n",
      "severe(flash flooding in palar and mahakamanta)\n",
      "severe (flash flooding)\n",
      "moderate(manu and khowai)\n",
      "severe(flooding of all major rivers)\n",
      "severe(all the major rivers)\n",
      "moderate(rapti)\n",
      "severe (ganga, yamuna and their tributaries)\n",
      "heavyh rains\n",
      "severe(ganga , yamuna, ken shandrawal, rapti, sarju, gadri and ghagra )\n",
      "moderate(tiata, torsa, raidak. mansi and dalong)\n",
      "moderate( mansi, mechhi and raidak)\n",
      "moderate (kajihi)\n",
      "heavy rains, severe\n",
      "heavy rains, moderate, severe\n",
      "heavy rains, flash flood\n",
      "heay rains\n",
      "heavy rains/tank burnt\n",
      "flash flood\n",
      "severe, heavy rains\n",
      "heavy rains \n",
      "severe flood, heavy rains\n",
      "heavy rains, severe flood\n",
      "severe flood (nea dihing)\n",
      "severe flood (brahmaputra and its tributaries)\n",
      "severe flood . (brahmaputra and its tributaries)\n",
      "severe flood (brahmaputra and its tributaries) \n",
      "severe flood (ganga and its tributaries)\n",
      "severe flood  (ganga and its tributaries) \n",
      "severe flood (narmada and its tributaries)\n",
      "severe food\n",
      "severe flood (doyang)\n",
      "severe flood (bias, satlaj and ravi)\n",
      "severe flood(ganga and its  tributaries) \n",
      "heav rains\n",
      "severe flood \n",
      "heavy rains , severe flood\n",
      "flash flood, heavy rain\n",
      "moderate flood \n",
      "heavy  rains\n",
      "moderate flood\n",
      "flash floods \n",
      "flash flood due to cloud burst\n",
      "heavy rains and flash floods\n",
      "flash flood or moderate flood\n",
      "flash floods due to cloud burst\n",
      "flash flood \n",
      "flash flood(jia bhareli)\n",
      "flash flood(bamai)\n",
      "flash flood(singla)\n",
      "severe flood(brahmaputra and its tributaries)\n",
      "severe flood(adhwara, burhi gandak, gandak, kosi and pun pun\n",
      "flash flood (narmada and tapi)\n",
      "severe flood(tapi)\n",
      "moderate flood (imphal, iril, nambul, thoubal and wangiing\n",
      "floash flood(jinjiram andrangai)\n",
      "moderate flood(ganga, yamuna and sohali)\n",
      "flash flood(jiadhol)\n",
      "moderate flood(dhansiri, etc)\n",
      "flash flood(salengi)\n",
      "flash flood(jiabharali)\n",
      "flash flood(gavoru)\n",
      "flash flood(barpani)\n",
      "severe flood(bagmati, kosi and kamala balan)\n",
      "severe flood(ganga, bagmati, kosi, mahananda. adhwara, gandak and burhi gandak)\n",
      "severe flood( ganga and its tributaries)\n",
      "flash flood(narmada)\n",
      "flash flood(umshyrpi)\n",
      "severe flood(mahanadi and its triutaries)\n",
      "flash flood(barak, dhaleswari and kushiara)\n",
      " heavy rains\n",
      "flash flood(brahmaputra)\n",
      "flash flood(brahmaputra and its tributaries)\n",
      "severe flood(adhwara, bagmati, burhi gandak, gandak, ganga, ghagra, kamla balan, kosi, kreh, mahananda, punpun and sone)\n",
      "heavy rains(cloud burst)\n",
      "flash floods(noa dihing, digaru and darrang)\n",
      "severe flood(bijoy and khowai)\n",
      "flash flood(cloud burst)\n",
      "flood(brahmaputra and its tributaries, jiadhol, kaldia, plagadia, kalo and drani)\n",
      "severe flood(saktola and pahumora)\n",
      "flash flood(belgir)\n",
      "flash flood(kalajal)\n",
      "severe flood(brahmaputra)\n",
      "severe flood(brahmaputra and its tributaries, kaldia)\n",
      "severe flood(yamuna)\n",
      "severe flood(sutlej)\n",
      "severe flood(beas and sutlej)\n",
      "flash flood(ghatprabha)\n",
      "flash floods(krishna and bhima)\n",
      "severe flood(narmada)\n",
      "moderate flood (brahmani and baitarani)\n",
      "moderate flood (ravi) \n",
      "severe flood (ker)\n",
      "severe flood (ghaghara)\n",
      "flood (ganga, ghaghara, magari and tansa) \n",
      "flash flood (godavari and rivulets)\n",
      "severe flood (noona, tributaries of brahmaputra)\n",
      "flash flood (brahmaputra)\n",
      "flash flood (brahmaputra and its tributaries)\n",
      "flash flood (burigang, ghaghara)\n",
      "flash flood (bagmati, kamla balan, mahanadi, burhi gandak and adhwara)\n",
      "flash flood (tapi)\n",
      "flash flood (markanda)\n",
      "heavy rains (cloud burst)\n",
      "flash flood (tributaries of satlej)\n",
      "flash flood (cloud burst)\n",
      "flash flood (jhelum, chenab)\n",
      "flash flood (bheema, krishna and ghatprabha)\n",
      "flash flood(bheema and krishna) \n",
      "heavy rains.\n",
      "flash flood (pavna, varna, krishna, panchganga & tributaries) \n",
      "heavy rains(khari) \n",
      "flood (krishna, nagavalli, tungabhadra and vamsadhara)\n",
      "flash flood (dhal, jia and kumtia) \n",
      "severe flood (dikhow, desang, brahmaputra)\n",
      "flood (beki and puthimari)\n",
      "flood (dikhow)\n",
      "flash flood (bagmati)\n",
      "flood (cloud burst)\n",
      "flash flood (hemavathi and kapila)\n",
      "flash flood (palem)\n",
      "flood (vansadha)\n",
      "severe flood (jiadhal and semen)\n",
      "flash flood (jiadhal and semen)\n",
      "flash flood (jaidhal)\n",
      "severe flood (bharali, jaidhal, jia, semen)\n",
      "flash flood (singra, gabharu, pav)\n",
      "flash flood (tributory of ranganadi)\n",
      "flood (mangaldoi)\n",
      "flood (nanoi and sakotala)\n",
      "flash flood (brahmaputra & kaldiya)\n",
      "severe flood (brahmaputra and manas, pachnoi) \n",
      "flash flood (puthimeri and ranganadi)\n",
      "severe flood (kosi)\n",
      "flood (kasushalya)\n",
      "flash flood (ghaggar)\n",
      "severe flood (satluj, yamuna)\n",
      "flash flood (beas)\n",
      "flash flood (rawi and chenab)\n",
      "flash flood (ghatprabha, hiranyakeshi, kaveri, tunga and many other rivers)\n",
      "flood (indrayani, mula, panchganga, pavana)\n",
      "flood (bhima, bindusara and chandrabhaga)\n",
      "flood (godavari)\n",
      "flood (mahanadi and its tributaries)\n",
      "flood (beas, ghagar, sutlej)\n",
      "flood (gomati)\n",
      "flood (ghagra)\n",
      "flash flood (krishna)\n",
      "flash flood (bhorali and gangadhar)\n",
      "flash flood (singra)\n",
      "flood (gainadi )\n",
      "flash flood (brahmaputra and deshang)\n",
      "flash flood (kaldia)\n",
      "flood (sopai and bisaikata)\n",
      "flash flood (darika and tiak) \n",
      "flood (brahmputra)\n",
      "flood (jiadhal kumotia and samarajan)\n",
      "flash flood (boka and singra)\n",
      "moderate flood (cloud burst)\n",
      "flood (bhadra)\n",
      "flash flood (krishna, ghatprabha and malprabha)\n",
      "flood (yerala)\n",
      "flood (mahanadi)\n",
      "flash flood (baghmati)\n",
      "severe flood (cloud burst)\n",
      "flood (ghaghra)\n",
      "flood (ganga)\n",
      "flash flood (singra, jiadha)\n",
      "flood (bramhaputra and its tributaries, tiyak and kakajan)\n",
      "flood (kaldia, pahumara and tihy)\n",
      "flood (singra, dikrang and paroi)\n",
      "flood (brahmaputra and its tributaries)\n",
      "flood (bhogdoi, ranganodi and singra)\n",
      "flood (brahmaputra  and and its tributaries) \n",
      "flood (nishary)\n",
      "flood (gabhagru, jiadhan, pabhoi and singra)\n",
      "flood (bramhaputra, kapili and kolong)\n",
      "flood (brahmaputra and its tributaries) \n",
      "flood (buradiya, kaldiya and tihu) \n",
      "flood (singra)\n",
      "flood (gandok)\n",
      "flood (yamuna)\n",
      "flood (krishnavati)\n",
      "flood (ujh)\n",
      "flood (karamana, neyyak and vamanapuram) \n",
      "flood (satluj and yamuna)\n",
      "flood (ganga ballia, deoria and\n",
      "flood (ganga, yamuna and sarayu)\n",
      "flood (ramganga and kosi)\n",
      "flood (kaljani, pana)\n",
      "flood (torsha)\n",
      "flood ( ganga and phulhar)\n",
      "severe flood (brahmaputra and tributaries)\n",
      "flood (brahmaputra and tributaries)\n",
      "flood (tiyak, puthi)\n",
      "flood (doprang)\n",
      "flood (ghai, jiadhal)\n",
      "severe flood (narmada and sabarmati)\n",
      "severe flood (tapi and mahanadi)\n",
      "severe floodi) (narmada) \n",
      "flash flood (jhelum)\n",
      "flood (ghataprabha and krishna)\n",
      "flood(dhasan) \n",
      "flood (bina river)\n",
      "flood(jagbudi, kajali and vashishthi) \n",
      "flood (arjuna,  karli and terekhol) \n",
      "flood (jagbudi, kundalika, savitri and vashishthi)\n",
      "severe flood (mahanadi & its tributaries)\n",
      "flood (ghagra and sharada)\n",
      "flood (ramganga and yamuna)\n",
      "flood (brahmaputra and its tributories)\n",
      "flood (brahmaputra, puthimari and singra)\n",
      "flood (dikchu, lachen, lachung, rangpo and singtam)\n",
      "heavy rains (cloud burst) \n",
      "flash flood (brahmaputra and dibru rivers)\n",
      "heavy rains(cloud burst) \n",
      "belgaum \n",
      "flood \n",
      "heavy to very rainfall/flash floods (saraswati and mandakini rivers and its tributaries triggered by very heavy rainfall and lake burst), cloudburst on jun16 &17 jun, massive landslides and avalanches reported at multiple places. \n",
      "flood (singra and soansiri rivers)\n",
      "flood (gandaki and kosi)\n",
      "flood (jehlum,chenab, tawi)\n",
      "heavy rains (cloudburst) \n",
      "flood (baitrani, budhaba langa, subarnarekha and mahanadi)\n",
      "flood (ghagra, rapti, sarayu and sharda)\n",
      "heavy rains & flash flood\n",
      "heavy rains and flood\n",
      "heavy rains & floods\n",
      "heavy rains(massive landslide) \n",
      "massive landslide \n",
      "heavy rains(landslide)\n",
      "heavy rains & flood in sigin river.\n",
      "flood (janji river) \n",
      "flood ( brahmaputra and tributaries) \n",
      "landslide \n",
      "flood (kapili river) \n",
      "flood ( brahmaputra and tributaries)\n",
      "heavy rain \n",
      "flood( brahmaputra and tributaries)  \n",
      "heavy rain\n",
      "heavy rain & floods in punpun & falgu river. \n",
      "floods in ganga & tributaries\n",
      "heavy rain & flood.\n",
      "heavy rains & flood in sheonath river\n",
      "heavy rains  \n",
      "heavy rains & landslide\n",
      "heavy rains & cloud burst in shimla.\n",
      "heavy rains & cloudburst\n",
      "flash flood in tawi river & landslide\n",
      "heavy rains & floods in krishna river\n",
      "heavy rains, flood & landslides\n",
      "floods & heavy rains\n",
      "flood in tamas river\n",
      "heavy rains & flood in narmada, tapi wainganga\n",
      "flood in godavari\n",
      "flood in savitri river\n",
      "heavy rain & flash flood\n",
      "landslide\n",
      "landslip\n",
      "flood(kuzhithurai  and sitraru rivers)\n",
      "heavy rains & heavy rains &\n",
      "heavy rains & landslides\n",
      "heavy rains & flood in mahananda river\n"
     ]
    }
   ],
   "source": [
    "unique_main_causes = data['Main Cause'].unique()\n",
    "\n",
    "print(\"All unique values in 'Main Cause' column:\")\n",
    "for cause in unique_main_causes:\n",
    "    print(cause)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8793ac27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Main Cause main_cause_cleaned\n",
      "0           Flooding              flood\n",
      "1             floods              flood\n",
      "2     Heavy Rainfall     heavy rainfall\n",
      "3              Flood              flood\n",
      "4              Storm              storm\n",
      "5            Cyclone              storm\n",
      "6          Windstorm              storm\n",
      "7         Earthquake         earthquake\n",
      "8   Seismic Activity         earthquake\n",
      "9               Fire               fire\n",
      "10       Forest Fire               fire\n",
      "11          Wildfire               fire\n",
      "12         Heat Wave          heat wave\n",
      "13          Heatwave          heat wave\n",
      "14        Heavy Rain     heavy rainfall\n",
      "15         Hailstorm              storm\n",
      "16         Snowstorm          snowstorm\n",
      "17         Avalanche          snowstorm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming your data is in a pandas DataFrame called df\n",
    "# Replace 'main_cause' with the actual name of your column if it's different\n",
    "df = pd.DataFrame({\n",
    "    'Main Cause': [\n",
    "        'Flooding', 'floods', 'Heavy Rainfall', 'Flood', 'Storm', \n",
    "        'Cyclone', 'Windstorm', 'Earthquake', 'Seismic Activity',\n",
    "        'Fire', 'Forest Fire', 'Wildfire', 'Heat Wave', 'Heatwave',\n",
    "        'Heavy Rain', 'Hailstorm', 'Snowstorm', 'Avalanche'\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Step 1: Convert all text to lowercase to standardize\n",
    "df['main_cause_cleaned'] = df['Main Cause'].str.lower()\n",
    "\n",
    "# Step 2: Define a dictionary to group similar categories\n",
    "group_mapping = {\n",
    "    'flood': ['flooding', 'floods', 'flood'],\n",
    "    'heavy rainfall': ['heavy rainfall', 'heavy rain'],\n",
    "    'storm': ['storm', 'cyclone', 'windstorm', 'hailstorm'],\n",
    "    'earthquake': ['earthquake', 'seismic activity'],\n",
    "    'fire': ['fire', 'forest fire', 'wildfire'],\n",
    "    'heat wave': ['heat wave', 'heatwave'],\n",
    "    'snowstorm': ['snowstorm', 'avalanche']\n",
    "}\n",
    "\n",
    "# Step 3: Create a function to map similar values\n",
    "def map_categories(value, group_mapping):\n",
    "    for key, group in group_mapping.items():\n",
    "        if value in group:\n",
    "            return key\n",
    "    return value  # return the original value if no match found\n",
    "\n",
    "# Apply the mapping function to standardize the main cause values\n",
    "df['main_cause_cleaned'] = df['main_cause_cleaned'].apply(map_categories, args=(group_mapping,))\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "print(df[['Main Cause', 'main_cause_cleaned']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9de76c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fuzzywuzzy in c:\\users\\rog\\anaconda3\\lib\\site-packages (0.18.0)\n",
      "Collecting python-Levenshtein\n",
      "  Obtaining dependency information for python-Levenshtein from https://files.pythonhosted.org/packages/23/83/fcf9f1a8f20e9c4b73f348c8ae881e6a328e475ea9488da52a736f7fd577/python_Levenshtein-0.26.0-py3-none-any.whl.metadata\n",
      "  Downloading python_Levenshtein-0.26.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting Levenshtein==0.26.0 (from python-Levenshtein)\n",
      "  Obtaining dependency information for Levenshtein==0.26.0 from https://files.pythonhosted.org/packages/ea/73/b5031d0f59b24da57a47a531a6f104d28c71050bad04ff30a53d9f55c15d/levenshtein-0.26.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading levenshtein-0.26.0-cp311-cp311-win_amd64.whl.metadata (3.2 kB)\n",
      "Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.26.0->python-Levenshtein)\n",
      "  Obtaining dependency information for rapidfuzz<4.0.0,>=3.9.0 from https://files.pythonhosted.org/packages/60/48/1bdb509cc379a15fc70bb23297ad7f11bf8c5a5f14d33fbc1a657d58f1f4/rapidfuzz-3.10.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading rapidfuzz-3.10.0-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Downloading python_Levenshtein-0.26.0-py3-none-any.whl (9.4 kB)\n",
      "Downloading levenshtein-0.26.0-cp311-cp311-win_amd64.whl (98 kB)\n",
      "   ---------------------------------------- 0.0/98.5 kB ? eta -:--:--\n",
      "   ------------ --------------------------- 30.7/98.5 kB 660.6 kB/s eta 0:00:01\n",
      "   ------------ --------------------------- 30.7/98.5 kB 660.6 kB/s eta 0:00:01\n",
      "   ------------ --------------------------- 30.7/98.5 kB 660.6 kB/s eta 0:00:01\n",
      "   ------------------------------------- -- 92.2/98.5 kB 525.1 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 98.5/98.5 kB 513.2 kB/s eta 0:00:00\n",
      "Downloading rapidfuzz-3.10.0-cp311-cp311-win_amd64.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.1/1.6 MB 1.1 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 0.2/1.6 MB 1.2 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 0.2/1.6 MB 1.1 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 0.2/1.6 MB 1.2 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 0.3/1.6 MB 1.1 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 0.3/1.6 MB 1.1 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 0.4/1.6 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 0.4/1.6 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 0.4/1.6 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 0.4/1.6 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 0.4/1.6 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 0.4/1.6 MB 1.1 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 0.6/1.6 MB 966.7 kB/s eta 0:00:02\n",
      "   --------------- ------------------------ 0.6/1.6 MB 922.8 kB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 0.7/1.6 MB 897.2 kB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 0.7/1.6 MB 914.8 kB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 0.7/1.6 MB 881.7 kB/s eta 0:00:02\n",
      "   ------------------ --------------------- 0.7/1.6 MB 861.4 kB/s eta 0:00:02\n",
      "   ------------------- -------------------- 0.8/1.6 MB 847.7 kB/s eta 0:00:01\n",
      "   ------------------- -------------------- 0.8/1.6 MB 824.3 kB/s eta 0:00:01\n",
      "   -------------------- ------------------- 0.8/1.6 MB 832.5 kB/s eta 0:00:01\n",
      "   -------------------- ------------------- 0.8/1.6 MB 832.5 kB/s eta 0:00:01\n",
      "   -------------------- ------------------- 0.8/1.6 MB 832.5 kB/s eta 0:00:01\n",
      "   -------------------- ------------------- 0.8/1.6 MB 832.5 kB/s eta 0:00:01\n",
      "   -------------------- ------------------- 0.8/1.6 MB 832.5 kB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 0.9/1.6 MB 764.4 kB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.0/1.6 MB 751.4 kB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.0/1.6 MB 748.7 kB/s eta 0:00:01\n",
      "   ------------------------ --------------- 1.0/1.6 MB 739.0 kB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.0/1.6 MB 727.3 kB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.0/1.6 MB 727.3 kB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.1/1.6 MB 722.7 kB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.1/1.6 MB 727.2 kB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.2/1.6 MB 719.5 kB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.2/1.6 MB 711.1 kB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.2/1.6 MB 715.5 kB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.3/1.6 MB 713.9 kB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.3/1.6 MB 713.9 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.3/1.6 MB 705.2 kB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.3/1.6 MB 693.1 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.3/1.6 MB 692.6 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.4/1.6 MB 686.9 kB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.4/1.6 MB 681.0 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.4/1.6 MB 670.2 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.4/1.6 MB 669.8 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.5/1.6 MB 664.8 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.5/1.6 MB 664.7 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.5/1.6 MB 655.4 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.5/1.6 MB 655.4 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.5/1.6 MB 655.4 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.5/1.6 MB 655.4 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.5/1.6 MB 655.4 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.5/1.6 MB 655.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.6/1.6 MB 636.3 kB/s eta 0:00:00\n",
      "Installing collected packages: rapidfuzz, Levenshtein, python-Levenshtein\n",
      "Successfully installed Levenshtein-0.26.0 python-Levenshtein-0.26.0 rapidfuzz-3.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip install fuzzywuzzy\n",
    "\n",
    "!pip install python-Levenshtein\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fff1597e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object, got 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 36\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Step 3: Apply the functions to clean the data\u001b[39;00m\n\u001b[0;32m     35\u001b[0m common_causes \u001b[38;5;241m=\u001b[39m get_most_common_causes(df, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMain Cause\u001b[39m\u001b[38;5;124m'\u001b[39m, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m---> 36\u001b[0m df_cleaned \u001b[38;5;241m=\u001b[39m group_similar_causes(df, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMain Cause\u001b[39m\u001b[38;5;124m'\u001b[39m, common_causes)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Step 4: Print the cleaned data to verify the results\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_cleaned[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMain Cause\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCleaned Cause\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n",
      "Cell \u001b[1;32mIn[9], line 21\u001b[0m, in \u001b[0;36mgroup_similar_causes\u001b[1;34m(df, column, common_causes, similarity_threshold)\u001b[0m\n\u001b[0;32m     18\u001b[0m unique_causes \u001b[38;5;241m=\u001b[39m df[column]\u001b[38;5;241m.\u001b[39munique()\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cause \u001b[38;5;129;01min\u001b[39;00m unique_causes:\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# Find the best match among common causes\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m     match, score \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mextractOne(cause, common_causes, scorer\u001b[38;5;241m=\u001b[39mfuzz\u001b[38;5;241m.\u001b[39mtoken_sort_ratio)\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# If the match is above the similarity threshold, group it with the common cause\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m score \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m similarity_threshold:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\fuzzywuzzy\\process.py:220\u001b[0m, in \u001b[0;36mextractOne\u001b[1;34m(query, choices, processor, scorer, score_cutoff)\u001b[0m\n\u001b[0;32m    218\u001b[0m best_list \u001b[38;5;241m=\u001b[39m extractWithoutOrder(query, choices, processor, scorer, score_cutoff)\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmax\u001b[39m(best_list, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m i: i[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\fuzzywuzzy\\process.py:78\u001b[0m, in \u001b[0;36mextractWithoutOrder\u001b[1;34m(query, choices, processor, scorer, score_cutoff)\u001b[0m\n\u001b[0;32m     75\u001b[0m     processor \u001b[38;5;241m=\u001b[39m no_process\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# Run the processor on the input query.\u001b[39;00m\n\u001b[1;32m---> 78\u001b[0m processed_query \u001b[38;5;241m=\u001b[39m processor(query)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(processed_query) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     81\u001b[0m     logging\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mu\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApplied processor reduces input query to empty string, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     82\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall comparisons will have score 0. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     83\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Query: \u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(query))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\fuzzywuzzy\\utils.py:95\u001b[0m, in \u001b[0;36mfull_process\u001b[1;34m(s, force_ascii)\u001b[0m\n\u001b[0;32m     93\u001b[0m     s \u001b[38;5;241m=\u001b[39m asciidammit(s)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# Keep only Letters and Numbers (see Unicode docs).\u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m string_out \u001b[38;5;241m=\u001b[39m StringProcessor\u001b[38;5;241m.\u001b[39mreplace_non_letters_non_numbers_with_whitespace(s)\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# Force into lowercase.\u001b[39;00m\n\u001b[0;32m     97\u001b[0m string_out \u001b[38;5;241m=\u001b[39m StringProcessor\u001b[38;5;241m.\u001b[39mto_lower_case(string_out)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\fuzzywuzzy\\string_processing.py:26\u001b[0m, in \u001b[0;36mStringProcessor.replace_non_letters_non_numbers_with_whitespace\u001b[1;34m(cls, a_string)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreplace_non_letters_non_numbers_with_whitespace\u001b[39m(\u001b[38;5;28mcls\u001b[39m, a_string):\n\u001b[0;32m     22\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;124;03m    This function replaces any sequence of non letters and non\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124;03m    numbers with a single white space.\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mregex\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, a_string)\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object, got 'float'"
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "df = pd.read_excel(file_path, engine='openpyxl')\n",
    "def get_most_common_causes(df, column, threshold=10):\n",
    "    # Count frequency of each unique cause\n",
    "    cause_counts = df[column].value_counts()\n",
    "    \n",
    "    # Get the most frequent causes (those appearing more than 'threshold' times)\n",
    "    common_causes = cause_counts[cause_counts >= threshold].index.tolist()\n",
    "    return common_causes\n",
    "\n",
    "# Step 2: Group less common causes with the most common similar ones\n",
    "def group_similar_causes(df, column, common_causes, similarity_threshold=80):\n",
    "    # Create a mapping for causes\n",
    "    cause_mapping = {}\n",
    "    \n",
    "    # Iterate over all unique causes\n",
    "    unique_causes = df[column].unique()\n",
    "    for cause in unique_causes:\n",
    "        # Find the best match among common causes\n",
    "        match, score = process.extractOne(cause, common_causes, scorer=fuzz.token_sort_ratio)\n",
    "        \n",
    "        # If the match is above the similarity threshold, group it with the common cause\n",
    "        if score >= similarity_threshold:\n",
    "            cause_mapping[cause] = match\n",
    "        else:\n",
    "            # If no good match is found, keep it as a separate cause\n",
    "            cause_mapping[cause] = cause\n",
    "    \n",
    "    # Apply the mapping to the dataframe\n",
    "    df['Cleaned Cause'] = df[column].map(cause_mapping)\n",
    "    return df\n",
    "\n",
    "# Step 3: Apply the functions to clean the data\n",
    "common_causes = get_most_common_causes(df, 'Main Cause', threshold=10)\n",
    "df_cleaned = group_similar_causes(df, 'Main Cause', common_causes)\n",
    "\n",
    "# Step 4: Print the cleaned data to verify the results\n",
    "print(df_cleaned[['Main Cause', 'Cleaned Cause']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c37b5f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  Main Cause  \\\n",
      "0                                 Heavy rain   \n",
      "1                                 Heavy rain   \n",
      "2                                 Heavy rain   \n",
      "3                           Tropical cyclone   \n",
      "4                             Monsoonal rain   \n",
      "...                                      ...   \n",
      "4733                               landslide   \n",
      "4734                heavy rains & landslides   \n",
      "4735                                   flood   \n",
      "4736                             heavy rains   \n",
      "4737  heavy rains & flood in mahananda river   \n",
      "\n",
      "                               Cleaned Cause  \n",
      "0                                 Heavy rain  \n",
      "1                                 Heavy rain  \n",
      "2                                 Heavy rain  \n",
      "3                           Tropical cyclone  \n",
      "4                             Monsoonal rain  \n",
      "...                                      ...  \n",
      "4733                               landslide  \n",
      "4734                heavy rains & landslides  \n",
      "4735                                   flood  \n",
      "4736                             heavy rains  \n",
      "4737  heavy rains & flood in mahananda river  \n",
      "\n",
      "[4738 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# Step 1: Read the data\n",
    "df = pd.read_excel(file_path, engine='openpyxl')\n",
    "\n",
    "# Step 2: Convert all entries in the 'Main Cause' column to strings and handle NaN values\n",
    "df['Main Cause'] = df['Main Cause'].fillna('Unknown').astype(str)\n",
    "\n",
    "# Step 3: Define function to find most common causes\n",
    "def get_most_common_causes(df, column, threshold=10):\n",
    "    # Count frequency of each unique cause\n",
    "    cause_counts = df[column].value_counts()\n",
    "    \n",
    "    # Get the most frequent causes (those appearing more than 'threshold' times)\n",
    "    common_causes = cause_counts[cause_counts >= threshold].index.tolist()\n",
    "    return common_causes\n",
    "\n",
    "# Step 4: Group less common causes with the most common similar ones\n",
    "def group_similar_causes(df, column, common_causes, similarity_threshold=80):\n",
    "    # Create a mapping for causes\n",
    "    cause_mapping = {}\n",
    "    \n",
    "    # Iterate over all unique causes\n",
    "    unique_causes = df[column].unique()\n",
    "    for cause in unique_causes:\n",
    "        # Ensure that the cause is a string\n",
    "        if isinstance(cause, str):\n",
    "            # Find the best match among common causes\n",
    "            result = process.extractOne(cause, common_causes, scorer=fuzz.token_sort_ratio)\n",
    "            if result:\n",
    "                match, score = result\n",
    "                # If the match is above the similarity threshold, group it with the common cause\n",
    "                if score >= similarity_threshold:\n",
    "                    cause_mapping[cause] = match\n",
    "                else:\n",
    "                    # If no good match is found, keep it as a separate cause\n",
    "                    cause_mapping[cause] = cause\n",
    "        else:\n",
    "            # If cause is not a string, assign it directly\n",
    "            cause_mapping[cause] = cause\n",
    "    \n",
    "    # Apply the mapping to the dataframe\n",
    "    df['Cleaned Cause'] = df[column].map(cause_mapping)\n",
    "    return df\n",
    "\n",
    "# Step 5: Apply the functions to clean the data\n",
    "common_causes = get_most_common_causes(df, 'Main Cause', threshold=10)\n",
    "df_cleaned = group_similar_causes(df, 'Main Cause', common_causes)\n",
    "\n",
    "# Step 6: Print the cleaned data to verify the results\n",
    "print(df_cleaned[['Main Cause', 'Cleaned Cause']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a78749a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 51\u001b[0m\n\u001b[0;32m     49\u001b[0m common_causes \u001b[38;5;241m=\u001b[39m get_most_common_causes(df, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMain Cause\u001b[39m\u001b[38;5;124m'\u001b[39m, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     50\u001b[0m unwanted_words \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mriver\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmahanadi\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mganges\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbrahmaputra\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Add any unwanted rare words here\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m df_cleaned \u001b[38;5;241m=\u001b[39m group_similar_causes(df, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMain Cause\u001b[39m\u001b[38;5;124m'\u001b[39m, common_causes, unwanted_words\u001b[38;5;241m=\u001b[39munwanted_words)\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Step 6: Print the cleaned data to verify results\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_cleaned[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMain Cause\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCleaned Cause\u001b[39m\u001b[38;5;124m'\u001b[39m]])\n",
      "Cell \u001b[1;32mIn[11], line 29\u001b[0m, in \u001b[0;36mgroup_similar_causes\u001b[1;34m(df, column, common_causes, similarity_threshold, unwanted_words)\u001b[0m\n\u001b[0;32m     26\u001b[0m cleaned_cause \u001b[38;5;241m=\u001b[39m clean_unwanted_words(cause, unwanted_words)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Split causes containing '&', 'and', or commas into multiple components\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m split_causes \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[&,]\u001b[39m\u001b[38;5;124m'\u001b[39m, cleaned_cause\u001b[38;5;241m.\u001b[39mlower())\n\u001b[0;32m     31\u001b[0m grouped_cause \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m split_cause \u001b[38;5;129;01min\u001b[39;00m split_causes:\n",
      "\u001b[1;31mNameError\u001b[0m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from fuzzywuzzy import fuzz, process\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "df = pd.read_excel(file_path, engine='openpyxl')\n",
    "\n",
    "# Step 2: Define function to get the most common causes\n",
    "def get_most_common_causes(df, column, threshold=10):\n",
    "    cause_counts = df[column].value_counts()\n",
    "    common_causes = cause_counts[cause_counts >= threshold].index.tolist()\n",
    "    return common_causes\n",
    "\n",
    "# Step 3: Define a function to clean unnecessary words\n",
    "def clean_unwanted_words(cause, unwanted_words):\n",
    "    for word in unwanted_words:\n",
    "        cause = cause.replace(word, \"\").strip()  # Remove unwanted words and extra spaces\n",
    "    return cause\n",
    "\n",
    "# Step 4: Group similar causes with the most common ones, also handle multiple causes in a single entry\n",
    "def group_similar_causes(df, column, common_causes, similarity_threshold=80, unwanted_words=[]):\n",
    "    cause_mapping = {}\n",
    "    unique_causes = df[column].unique()\n",
    "    \n",
    "    for cause in unique_causes:\n",
    "        # Clean the cause by removing unwanted words\n",
    "        cleaned_cause = clean_unwanted_words(cause, unwanted_words)\n",
    "        \n",
    "        # Split causes containing '&', 'and', or commas into multiple components\n",
    "        split_causes = re.split(r'[&,]', cleaned_cause.lower())\n",
    "        \n",
    "        grouped_cause = []\n",
    "        for split_cause in split_causes:\n",
    "            split_cause = split_cause.strip()  # Remove extra spaces\n",
    "            match, score = process.extractOne(split_cause, common_causes, scorer=fuzz.token_sort_ratio)\n",
    "            \n",
    "            if score >= similarity_threshold:\n",
    "                grouped_cause.append(match)  # Group with common cause\n",
    "        \n",
    "        # Create a combined list of the matched causes\n",
    "        if grouped_cause:\n",
    "            cause_mapping[cause] = ', '.join(sorted(set(grouped_cause)))\n",
    "        else:\n",
    "            cause_mapping[cause] = cleaned_cause  # Keep as it is if no matches found\n",
    "    \n",
    "    df['Cleaned Cause'] = df[column].map(cause_mapping)\n",
    "    return df\n",
    "\n",
    "# Step 5: Apply the function to clean the data\n",
    "common_causes = get_most_common_causes(df, 'Main Cause', threshold=10)\n",
    "unwanted_words = ['river', 'mahanadi', 'ganges', 'brahmaputra']  # Add any unwanted rare words here\n",
    "df_cleaned = group_similar_causes(df, 'Main Cause', common_causes, unwanted_words=unwanted_words)\n",
    "\n",
    "# Step 6: Print the cleaned data to verify results\n",
    "print(df_cleaned[['Main Cause', 'Cleaned Cause']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0832c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from fuzzywuzzy import fuzz, process\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7dff0b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path\n",
    "file_path = \"Copy of IndianFloodInventroy(IFI)(1).xlsx\"\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_excel(file_path, engine='openpyxl')\n",
    "\n",
    "# Convert all entries in 'Main Cause' to lowercase strings and handle NaN values\n",
    "df['Main Cause'] = df['Main Cause'].fillna('Unknown').astype(str).str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c08ed0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of common words (frequency >= 5): 54\n"
     ]
    }
   ],
   "source": [
    "# Function to tokenize and count word frequencies\n",
    "def get_word_frequencies(df, column):\n",
    "    # Extract words using regex\n",
    "    all_words = df[column].apply(lambda x: re.findall(r'\\b\\w+\\b', x))\n",
    "    # Flatten the list of lists and count word frequencies\n",
    "    word_counts = Counter([word for sublist in all_words for word in sublist])\n",
    "    return word_counts\n",
    "\n",
    "# Get word frequencies\n",
    "word_counts = get_word_frequencies(df, 'Main Cause')\n",
    "\n",
    "# Define a threshold for word frequency; words appearing less than this are considered uncommon\n",
    "word_threshold = 5  # You can adjust this number based on your data\n",
    "common_words = set([word for word, count in word_counts.items() if count >= word_threshold])\n",
    "\n",
    "print(f\"Number of common words (frequency >= {word_threshold}): {len(common_words)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3234f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common causes (appearing >=10 times): ['heavy rains', 'severe', 'flash flood', 'heavy rains ', 'monsoonal rain', 'riverine flood', 'unknown', 'severe flood', 'heavy rain', 'severe floods', 'floods', 'flood', 'flash floods', 'moderate floods', 'moderate', 'cloud burst', 'moderate flood', 'torrential rain', 'moderate to severe', 'tropical cyclone', 'moderate flood ', 'severe floods ', 'heavy rains (cloud burst)']\n",
      "Common words (appearing >=5 times): {'singra', 'river', 'ghagra', 'brahmaputra', 'narmada', 'of', 'to', 'mahananda', 'godavari', 'break', 'in', 'yamuna', 'dam', 'heavy', 'burst', 'tapi', 'release', 'rivers', 'levy', 'floods', 'ghatprabha', 'their', 'cloud', 'gandak', 'unseasonal', 'bagmati', 'cyclone', 'tributories', 'flooding', 'rains', 'burhi', 'unknown', 'and', 'moderate', 'riverine', 'tributaries', 'bhima', 'flash', 'monsoonal', 'mahanadi', 'severe', 'torrential', 'krishna', 'tropical', 'ganga', 'rain', 'or', 'its', 'due', 'landslide', 'rapti', 'flood', 'incessant', 'kosi'}\n"
     ]
    }
   ],
   "source": [
    "# Function to identify common causes based on word frequencies\n",
    "def get_common_causes(df, column, word_threshold=5, freq_threshold=10):\n",
    "    # Get word frequencies\n",
    "    word_counts = get_word_frequencies(df, column)\n",
    "    \n",
    "    # Identify common words\n",
    "    common_words = set([word for word, count in word_counts.items() if count >= word_threshold])\n",
    "    \n",
    "    # Count frequency of each unique 'Main Cause' entry\n",
    "    cause_counts = df[column].value_counts()\n",
    "    \n",
    "    # Select 'Main Cause' entries that appear more than freq_threshold times\n",
    "    common_causes = cause_counts[cause_counts >= freq_threshold].index.tolist()\n",
    "    \n",
    "    return common_causes, common_words\n",
    "\n",
    "# Get common causes and common words\n",
    "common_causes, common_words = get_common_causes(df, 'Main Cause', word_threshold=5, freq_threshold=10)\n",
    "\n",
    "print(f\"Common causes (appearing >=10 times): {common_causes}\")\n",
    "print(f\"Common words (appearing >=5 times): {common_words}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1d05cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean and map causes\n",
    "def clean_and_map_causes(df, column, common_causes, common_words, similarity_threshold=80):\n",
    "    cause_mapping = {}\n",
    "    unique_causes = df[column].unique()\n",
    "    \n",
    "    for cause in unique_causes:\n",
    "        # Skip 'unknown' entries\n",
    "        if cause == 'unknown':\n",
    "            cause_mapping[cause] = 'Unknown'\n",
    "            continue\n",
    "        \n",
    "        # Split the cause into individual components if it contains '&', 'and', or ','\n",
    "        split_causes = re.split(r'[&, and]+', cause)\n",
    "        \n",
    "        mapped_causes = []\n",
    "        for single_cause in split_causes:\n",
    "            # Remove unwanted words by keeping only common words\n",
    "            words = re.findall(r'\\b\\w+\\b', single_cause)\n",
    "            cleaned_words = [word for word in words if word in common_words]\n",
    "            cleaned_cause = ' '.join(cleaned_words)\n",
    "            \n",
    "            if cleaned_cause:\n",
    "                # Find the best match among common causes using fuzzy matching\n",
    "                match = process.extractOne(cleaned_cause, common_causes, scorer=fuzz.token_sort_ratio)\n",
    "                if match:\n",
    "                    matched_cause, score = match\n",
    "                    if score >= similarity_threshold:\n",
    "                        mapped_causes.append(matched_cause)\n",
    "        \n",
    "        # Remove duplicates and sort\n",
    "        mapped_cause_unique = sorted(set(mapped_causes))\n",
    "        \n",
    "        if mapped_cause_unique:\n",
    "            # Join multiple causes with a comma\n",
    "            cause_mapping[cause] = ', '.join(mapped_cause_unique)\n",
    "        else:\n",
    "            # If no mapping found, label as 'Other'\n",
    "            cause_mapping[cause] = 'Other'\n",
    "    \n",
    "    # Apply the mapping to the dataframe\n",
    "    df['Cleaned Cause'] = df[column].map(cause_mapping)\n",
    "    return df\n",
    "\n",
    "# Apply the cleaning and mapping function\n",
    "df_cleaned = clean_and_map_causes(df, 'Main Cause', common_causes, common_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56a0e5ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Main Cause Cleaned Cause\n",
      "0         heavy rain         Other\n",
      "1         heavy rain         Other\n",
      "2         heavy rain         Other\n",
      "3   tropical cyclone         Other\n",
      "4     monsoonal rain         Other\n",
      "5     monsoonal rain         Other\n",
      "6         heavy rain         Other\n",
      "7     monsoonal rain         Other\n",
      "8         heavy rain         Other\n",
      "9     monsoonal rain         Other\n",
      "10        heavy rain         Other\n",
      "11        heavy rain         Other\n",
      "12    monsoonal rain         Other\n",
      "13        heavy rain         Other\n",
      "14        heavy rain         Other\n",
      "15    monsoonal rain         Other\n",
      "16    monsoonal rain         Other\n",
      "17        heavy rain         Other\n",
      "18   torrential rain         Other\n",
      "19  tropical cyclone         Other\n"
     ]
    }
   ],
   "source": [
    "# Display a sample of the cleaned data\n",
    "print(df_cleaned[['Main Cause', 'Cleaned Cause']].head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a97a484e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Heavy rain' 'Tropical cyclone' 'Monsoonal rain' 'Torrential rain'\n",
      " 'Dam/Levy break' 'Riverine flood' None 'Flash flood' 'Coastal flood'\n",
      " 'Flood' 'Heavy rain and flood' 'Other' 'Rain' 'Incessant rain'\n",
      " 'moderate rain' 'River flood (specific)' 'Unseasonal rain' 'River flood'\n",
      " 'Heavy rain and landslide' 'Landslide']\n",
      "Heavy rain                  3191\n",
      "Flood                        524\n",
      "Flash flood                  317\n",
      "Riverine flood               166\n",
      "Monsoonal rain               159\n",
      "moderate rain                 82\n",
      "Heavy rain and flood          45\n",
      "Other                         30\n",
      "Torrential rain               25\n",
      "Tropical cyclone              18\n",
      "Rain                           9\n",
      "Dam/Levy break                 8\n",
      "Landslide                      8\n",
      "Heavy rain and landslide       7\n",
      "Coastal flood                  4\n",
      "Incessant rain                 4\n",
      "River flood (specific)         1\n",
      "Unseasonal rain                1\n",
      "River flood                    1\n",
      "Name: Main Cause Cleaned, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = \"Copy of IndianFloodInventroy(IFI)(1).xlsx\"  # Replace with your file path\n",
    "df = pd.read_excel(file_path, sheet_name='IndianFloodInventory')\n",
    "\n",
    "# Function to clean and categorize the 'Main Cause' column\n",
    "def clean_main_cause(cause):\n",
    "    if pd.isna(cause):\n",
    "        return None\n",
    "    \n",
    "    # Convert to lowercase and remove extra spaces\n",
    "    cause = cause.strip().lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    cause = re.sub(r'[^\\w\\s]', '', cause)\n",
    "    \n",
    "    # Simplify redundant phrases and group categories\n",
    "    if 'rain' in cause:\n",
    "        if 'heavy rain' in cause or 'heavy rains' in cause:\n",
    "            if 'flood' in cause:\n",
    "                return 'Heavy rain and flood'\n",
    "            if 'landslide' in cause:\n",
    "                return 'Heavy rain and landslide'\n",
    "            return 'Heavy rain'\n",
    "        if 'monsoonal' in cause:\n",
    "            return 'Monsoonal rain'\n",
    "        if 'incessant' in cause:\n",
    "            return 'Incessant rain'\n",
    "        if 'torrential' in cause:\n",
    "            return 'Torrential rain'\n",
    "        if 'unseasonal' in cause:\n",
    "            return 'Unseasonal rain'\n",
    "        return 'Rain'\n",
    "    \n",
    "    if 'flood' in cause:\n",
    "        if 'flash flood' in cause:\n",
    "            return 'Flash flood'\n",
    "        if 'river' in cause:\n",
    "            return 'Riverine flood'\n",
    "        if 'coastal' in cause:\n",
    "            return \"Coastal flood\"\n",
    "        # Handle any mention of rivers alongside floods\n",
    "        if re.search(r'(river|rivulet|tributary|stream)', cause):\n",
    "            return 'River flood'\n",
    "        return 'Flood'\n",
    "    \n",
    "    if 'cyclone' in cause:\n",
    "        if 'tropical' in cause:\n",
    "            return 'Tropical cyclone'\n",
    "        if 'extra' in cause:\n",
    "            return 'Extra-tropical cyclone'\n",
    "    \n",
    "    if 'landslide' in cause or 'landslip' in cause:\n",
    "        return 'Landslide'\n",
    "    \n",
    "    if 'dam' in cause or 'levy' in cause:\n",
    "        return 'Dam/Levy break'\n",
    "    \n",
    "    if 'moderate' in cause:\n",
    "        return 'moderate rain'\n",
    "    if  'severe' in cause:\n",
    "        return 'Heavy rain'\n",
    "    \n",
    "    # Special handling for specific river names (like mahananda, brahmaputra, etc.)\n",
    "    if re.search(r'(mahananda|brahmaputra|ganga|godavari|narmada|tapi|wainganga|kosi|pennar)', cause):\n",
    "        return 'River flood (specific)'\n",
    "    \n",
    "    # If none of the above matches, return 'Other'\n",
    "    return 'Other'\n",
    "\n",
    "# Apply the cleaning function to the 'Main Cause' column\n",
    "df['Main Cause Cleaned'] = df['Main Cause'].apply(clean_main_cause)\n",
    "\n",
    "\n",
    "# Display the cleaned 'Main Cause' values\n",
    "print(df['Main Cause Cleaned'].unique())\n",
    "print(df['Main Cause Cleaned'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c7b4c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 'NA' values in 'Main Cause': 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = \"Copy of IndianFloodInventroy(IFI)(1).xlsx\"  # Replace with your file path\n",
    "df = pd.read_excel(file_path, sheet_name='IndianFloodInventory')\n",
    "\n",
    "# Count occurrences of 'NA' in the 'Main Cause' column\n",
    "na_count = df['Main Cause'].str.strip().str.upper().eq('NA').sum()\n",
    "\n",
    "# Print the count of 'NA' values\n",
    "print(f\"Number of 'NA' values in 'Main Cause': {na_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c660f86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Heavy rain                   2937\n",
      "Severity-based flood          575\n",
      "Riverine flood                388\n",
      "Flash flood                   256\n",
      "Monsoonal rain                159\n",
      "Flood                         104\n",
      "Heavy rain and flood           46\n",
      "cloud burst                    30\n",
      "Torrential rain                25\n",
      "Tropical cyclone               18\n",
      "River flood (specific)         12\n",
      "Heavy rain & Cloud Burst        9\n",
      "Landslide                       8\n",
      "Dam/Levy break                  8\n",
      "Heavy rain and landslide        7\n",
      "Other                           5\n",
      "Coastal flood                   4\n",
      "Flash flood & Cloud Burst       3\n",
      "Flood & Cloud Burst             1\n",
      "Unseasonal rain                 1\n",
      "River flood                     1\n",
      "Incessant rain                  1\n",
      "Rain                            1\n",
      "Flood & Landslide               1\n",
      "Name: Main Cause Cleaned 2, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = \"cleaned_flood_data_grouped.xlsx\"  # Replace with your file path\n",
    "df = pd.read_excel(file_path, sheet_name=\"Sheet1\")\n",
    "\n",
    "print(df['Main Cause Cleaned 2'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ecbe654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rain-related                 3186\n",
      "Severity-based flood          575\n",
      "Riverine flood                388\n",
      "Flash flood                   256\n",
      "Flood                         104\n",
      "cloud burst                    30\n",
      "Tropical cyclone               18\n",
      "River flood (specific)         12\n",
      "Dam/Levy break                  8\n",
      "Landslide                       8\n",
      "Other                           5\n",
      "Coastal flood                   4\n",
      "Flash flood & Cloud Burst       3\n",
      "Flood & Landslide               1\n",
      "Flood & Cloud Burst             1\n",
      "River flood                     1\n",
      "Name: New Cause Category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Function to group rain-related causes\n",
    "def group_rain_causes(cause):\n",
    "    if isinstance(cause, str) and 'rain' in cause.lower():  # Ensure cause is a string before checking\n",
    "        return 'Rain-related'\n",
    "    return cause\n",
    "\n",
    "# Apply the function to clean the 'Main Cause Cleaned 2' column\n",
    "df['New Cause Category'] = df['Main Cause Cleaned 2'].apply(group_rain_causes)\n",
    "\n",
    "\n",
    "print(df['New Cause Category'].value_counts())\n",
    "# Save the updated DataFrame back to Excel if needed\n",
    "# df.to_excel('updated_file.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4819697b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rain-related              3190\n",
      "Severity-based flood       575\n",
      "Riverine flood             388\n",
      "Flash flood                256\n",
      "Flood                      104\n",
      "cloud burst                 30\n",
      "Tropical cyclone            18\n",
      "River flood (specific)      12\n",
      "Dam/Levy break               8\n",
      "Landslide                    8\n",
      "Other                        5\n",
      "Coastal flood                4\n",
      "Flood & Landslide            1\n",
      "River flood                  1\n",
      "Name: New Cause Category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Function to update specific cases after the initial grouping\n",
    "def update_cloud_burst_and_flood(cause):\n",
    "    if isinstance(cause, str):\n",
    "        cause_lower = cause.lower()\n",
    "        if 'flash flood & cloud burst' in cause_lower or 'flood & cloud burst' in cause_lower:\n",
    "            return 'Rain-related'\n",
    "    return cause\n",
    "\n",
    "# Apply the function to update 'New Cause Category' with specific cases\n",
    "df['New Cause Category'] = df['New Cause Category'].apply(update_cloud_burst_and_flood)\n",
    "\n",
    "# Print the updated value counts\n",
    "print(df['New Cause Category'].value_counts())\n",
    "\n",
    "# Save the updated DataFrame back to Excel if needed\n",
    "# df.to_excel('updated_file.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99866d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rain-related            3190\n",
      "Severity-based flood     575\n",
      "Riverine flood           401\n",
      "Flash flood              256\n",
      "Flood                    104\n",
      "cloud burst               30\n",
      "Tropical cyclone          18\n",
      "Dam/Levy break             8\n",
      "Landslide                  8\n",
      "Other                      5\n",
      "Coastal flood              4\n",
      "Flood & Landslide          1\n",
      "Name: New Cause Category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Function to group 'River flood' and 'River flood (specific)' into 'Riverine flood'\n",
    "def update_river_flood(cause):\n",
    "    if isinstance(cause, str):\n",
    "        cause_lower = cause.lower()\n",
    "        if 'river flood' in cause_lower:  # Match both 'river flood' and 'river flood (specific)'\n",
    "            return 'Riverine flood'\n",
    "    return cause\n",
    "\n",
    "# Apply the function to update 'New Cause Category' with river flood cases\n",
    "df['New Cause Category'] = df['New Cause Category'].apply(update_river_flood)\n",
    "\n",
    "# Print the updated value counts\n",
    "print(df['New Cause Category'].value_counts())\n",
    "\n",
    "# Save the updated DataFrame back to Excel if needed\n",
    "# df.to_excel('updated_file.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92b5e93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rain-related            3220\n",
      "Severity-based flood     575\n",
      "Riverine flood           401\n",
      "Flash flood              256\n",
      "Flood                    104\n",
      "Tropical cyclone          18\n",
      "Landslide                  9\n",
      "Dam/Levy break             8\n",
      "Other                      5\n",
      "Coastal flood              4\n",
      "Name: New Cause Category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Function to group 'Flood & Landslide' into 'Landslide' and 'cloud burst' into 'Rain-related'\n",
    "def update_landslide_and_cloudburst(cause):\n",
    "    if isinstance(cause, str):\n",
    "        cause_lower = cause.lower()\n",
    "        if 'flood & landslide' in cause_lower:\n",
    "            return 'Landslide'\n",
    "        elif 'cloud burst' in cause_lower:  # Matches both standalone 'cloud burst' and combined cases already handled\n",
    "            return 'Rain-related'\n",
    "    return cause\n",
    "\n",
    "# Apply the function to update 'New Cause Category' with the new groupings\n",
    "df['New Cause Category'] = df['New Cause Category'].apply(update_landslide_and_cloudburst)\n",
    "\n",
    "# Print the updated value counts\n",
    "print(df['New Cause Category'].value_counts())\n",
    "\n",
    "# Save the updated DataFrame back to Excel if needed\n",
    "# df.to_excel('updated_file.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01376ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Cause values where 'New Cause Category' is 'Other':\n",
      "3759    Other\n",
      "3760    Other\n",
      "4323    Other\n",
      "4641    Other\n",
      "4644    Other\n",
      "Name: Main Cause Cleaned 2, dtype: object\n",
      "\n",
      "Unique 'Main Cause Cleaned 2' value counts where 'New Cause Category' is 'Severity-based flood':\n",
      "Severity-based flood    575\n",
      "Name: Main Cause Cleaned 2, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Display rows where 'New Cause Category' is 'Other' and show 'Main Cause Cleaned 2'\n",
    "other_values = df[df['New Cause Category'] == 'Other']['Main Cause Cleaned 2']\n",
    "print(\"Main Cause values where 'New Cause Category' is 'Other':\")\n",
    "print(other_values)\n",
    "\n",
    "# Display unique value counts where 'New Cause Category' is 'Severity-based flood'\n",
    "severity_flood_counts = df[df['New Cause Category'] == 'Severity-based flood']['Main Cause Cleaned 2'].value_counts()\n",
    "print(\"\\nUnique 'Main Cause Cleaned 2' value counts where 'New Cause Category' is 'Severity-based flood':\")\n",
    "print(severity_flood_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9c299d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated DataFrame back to the same Excel file\n",
    "df.to_excel('cleaned_flood_data_grouped.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2ce98e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
